{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e7f081",
   "metadata": {},
   "source": [
    "# **Machine Learning with Spotify: A Beginner's Guide to Decision Trees**\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Silhouette_of_bare_tree_branches_under_twilight_sky.jpg/640px-Silhouette_of_bare_tree_branches_under_twilight_sky.jpg\" width=\"400\">\n",
    "\n",
    "A Decision Tree is a powerful and popular tool in machine learning that works like a flowchart. It makes decisions by asking a series of simple \"if-then\" questions about the data. Think of it like a game of 20 questionsâ€”each question narrows down the possibilities until a final conclusion is reached.\n",
    "\n",
    "Decision trees are an excellent introduction to machine learning for several key reasons:\n",
    "\n",
    "- They are intuitive and easy to understand. The visual, flowchart-like structure makes it easy to see exactly how the model is making its decisions. You don't need complex math to follow the logic.\n",
    "\n",
    "- They mirror human thinking. The process of splitting data based on a series of questions is very similar to how people often make decisions in their daily lives.\n",
    "\n",
    "- They provide a great foundation. Understanding how a single decision tree works is the first step to learning more advanced and powerful models, like Random Forests.\n",
    "\n",
    "Decision trees are a type of supervised learning. This means we train the model on a dataset that already has the correct answers, or labels. The tree's job is to learn the relationship between the input data (the \"features\") and the known output labels, so it can make accurate predictions on new, unlabeled data.\n",
    "\n",
    "We will use a dataset from Spotify and build a **Decision Tree** model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efe4768",
   "metadata": {},
   "source": [
    "### **1. Understanding the Data**\n",
    "\n",
    "Spotify analyzes songs and gives them a set of attributes. In machine learning, we call these **features**. These features describe the characteristics of each song. Here are a few we'll be working with:\n",
    "\n",
    "* **`danceability`**: How suitable a track is for dancing (0.0 is least danceable, 1.0 is most danceable).\n",
    "* **`energy`**: A measure of intensity and activity (0.0 is low energy, 1.0 is high energy).\n",
    "* **`acousticness`**: A measure of whether the track is acoustic (0.0 is not acoustic, 1.0 is acoustic).\n",
    "* **`valence`**: A measure of musical positiveness (0.0 is negative/sad, 1.0 is positive/happy).\n",
    "* **`liveness`**: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live.\n",
    "* **`instrumentalness`**: Predicts whether a track contains no vocals. Values closer to 1.0 mean it's more likely to be an instrumental track.\n",
    "* **`speechiness`**: Detects the presence of spoken words. A higher value indicates the presence of more spoken words.\n",
    "\n",
    "Our goal is to use these features to predict whether a song is **country** or **rap**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59fb082",
   "metadata": {},
   "source": [
    "### **2. Setting Up Our Workspace**\n",
    "\n",
    "We will be using several libraries in this notebook:\n",
    "\n",
    "- `pandas`for data analysis and manipulation. \n",
    "\n",
    "- `matplotlib.pyplot` for creating visualizations. \n",
    "\n",
    "- scikit-learn (`sklearn`): The primary machine learning library in Python. We use several of its components:\n",
    "  - `model_selection`: To split our data into training, validation, and test sets\n",
    "  - `tree`: To build and visualize our `DecisionTreeClassifier`.\n",
    "  - `metrics`: To evaluate our model's performance using `accuracy_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d69d8-97bd-42b1-a3aa-90cf622ab414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54496820-5478-4f52-83e8-eba4a39b3f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Spotify dataset\n",
    "spotify = pd.read_csv('data/spotify.csv')\n",
    "\n",
    "# Let's look at the first few rows of our data\n",
    "spotify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique genres\n",
    "spotify['genre'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e243ae2-2585-4483-ba58-1762df332de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the summary statistics for the numerical data\n",
    "spotify.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63912a64",
   "metadata": {},
   "source": [
    "### **3. Preparing the Data for Machine Learning**\n",
    "\n",
    "A machine learning model learns from data. We need to tell it what we want to predict (the **target**) and what information it should use to make those predictions (the **features**).\n",
    "\n",
    "* **Target (`y`):** The single thing we want to predict. In our case, the `genre`.\n",
    "* **Features (`X`):** The data we use to predict the target. We'll select a few of the song attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36576d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our target variable\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Our feature variables\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Let's look at our features\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbbb281",
   "metadata": {},
   "source": [
    "#### **Splitting Our Data: Training, Validation, and Testing**\n",
    "\n",
    "This is one of the most important concepts in machine learning! To properly evaluate our model, we split our dataset into three parts:\n",
    "\n",
    "1.  **Training Set (70%):** The model will \"study\" this data to learn the patterns between the features and the song genre.\n",
    "2.  **Validation Set (20%):** We use this set to **tune** our model, for example, by finding the best settings (like tree depth) that give the highest accuracy.\n",
    "3.  **Test Set (10%):** This data is kept completely separate until the very end. We use it only once to get a final, unbiased measure of how our fully-tuned model performs on new, unseen songs.\n",
    "\n",
    "We can create this three-way split by using the `train_test_split` function twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f5eee-9685-4095-baaa-70f6dcc0a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split the data into a 70% training set and a 30% temporary set\n",
    "# We use 'stratify=y' to ensure the genre proportions are the same in the train and temp sets\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034d45e4-200c-4cfe-91c2-e288beae0cd2",
   "metadata": {},
   "source": [
    "Next, split the 30% temporary set into a 20% validation and 10% test set. The test set will be 1/3 of the temporary set (10% of the original total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f1cfc-c206-4d5a-b520-e63f628d25a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe565a75",
   "metadata": {},
   "source": [
    "### **4. Building and Visualizing Our First Decision Tree**\n",
    "\n",
    "Now for the fun part! We'll create a Decision Tree model. Think of a decision tree like a flowchart of \"if-then\" questions that the model learns to ask to determine a song's genre.\n",
    "\n",
    "To make it easy to understand, we'll start with a very simple tree by setting `max_leaf_nodes=5`. This means the tree can have at most 5 final decision points (leaves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple decision tree model\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Train the model on our training data\n",
    "simple_model.fit(X_train, y_train)\n",
    "\n",
    "# Let's visualize the tree!\n",
    "plt.figure(figsize=(20, 12))\n",
    "plot_tree(simple_model,\n",
    "          feature_names=features,\n",
    "          class_names=sorted(y.unique()),\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1638a7ab-76a7-4a5b-8461-8ab70436f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the validation data\n",
    "val_predictions = simple_model.predict(X_val)\n",
    "\n",
    "# Check the accuracy\n",
    "accuracy = accuracy_score(y_val, val_predictions)\n",
    "\n",
    "print(f\"Accuracy for a tree with 5 leaves: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4050f711-a8af-40df-bf44-00e93d606cff",
   "metadata": {},
   "source": [
    "### **5. Improving the Model: Finding the Right Complexity**\n",
    "\n",
    "Our first tree was very simple. A more complex tree might be more accurate, but a tree that is *too* complex can **overfit**â€”it learns the training data's noise instead of the true patterns. This causes it to perform poorly on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4122eb-4fa2-4231-9214-b5b8741acda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complex decision tree model\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Train the model on our training data\n",
    "complex_model.fit(X_train, y_train)\n",
    "\n",
    "# Let's visualize the tree!\n",
    "plt.figure(figsize=(50, 30))\n",
    "plot_tree(complex_model,\n",
    "          feature_names=features,\n",
    "          class_names=sorted(y.unique()),\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4985c9-c462-4f49-9545-0eb6ae2ffacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the validation data\n",
    "val_predictions = complex_model.predict(X_val)\n",
    "\n",
    "# Check the accuracy\n",
    "accuracy = accuracy_score(y_val, val_predictions)\n",
    "\n",
    "print(f\"Accuracy for a tree with 1000 leaves: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccef589-b8d9-4d2a-9c50-7c9913a1da28",
   "metadata": {},
   "source": [
    "Our goal is to find the \"sweet spot\" that balances simplicity and accuracy. We do this by iterating:\n",
    "1.  **Train** a model with a specific complexity (e.g., `max_leaf_nodes=20`).\n",
    "2.  **Validate** its performance on the validation set (`X_val`, `y_val`).\n",
    "3.  Repeat this for many different complexity settings.\n",
    "4.  Choose the setting that gave the best validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cc68cc",
   "metadata": {},
   "source": [
    "#### **Step 1 & 2: A Single Manual Iteration**\n",
    "\n",
    "Let's see how one cycle of this process works. We'll pick `max_leaf_nodes=20` and see what accuracy we get on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f152bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model with a specific number of leaf nodes\n",
    "model_20_leaves = DecisionTreeClassifier(max_leaf_nodes=20, random_state=1)\n",
    "\n",
    "\n",
    "# Train it on the training data\n",
    "model_20_leaves.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions on the validation data\n",
    "val_predictions = model_20_leaves.predict(X_val)\n",
    "\n",
    "# Check the accuracy\n",
    "accuracy = accuracy_score(y_val, val_predictions)\n",
    "\n",
    "print(f\"Accuracy for a tree with 20 leaves: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b6112",
   "metadata": {},
   "source": [
    "#### **Step 3: Automating the Iteration with a Loop**\n",
    "\n",
    "Doing this manually for every possible setting would be tedious. Let's create a loop to test a range of `max_leaf_nodes` values and store each result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704afd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of leaf node values to test\n",
    "candidate_max_leaf_nodes = list(range(5, 201, 10))\n",
    "validation_scores = {}\n",
    "\n",
    "for leaf_nodes in candidate_max_leaf_nodes:\n",
    "    # Create and train the model\n",
    "    model = DecisionTreeClassifier(max_leaf_nodes=leaf_nodes, random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions and score\n",
    "    val_preds = model.predict(X_val)\n",
    "    score = accuracy_score(y_val, val_preds)\n",
    "    \n",
    "    # Store the score\n",
    "    validation_scores[leaf_nodes] = score\n",
    "    \n",
    "    print(f\"Max leaf nodes: {leaf_nodes} \\t\\t Validation Accuracy: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050d5794",
   "metadata": {},
   "source": [
    "#### **Step 4: Visualizing the Results to Find the Sweet Spot**\n",
    "\n",
    "A great way to see the sweet spot is to plot the results. We are looking for the point where the accuracy is highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d544ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(validation_scores.keys()), list(validation_scores.values()), marker='o')\n",
    "plt.xlabel(\"Max Leaf Nodes\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Finding the Optimal Tree Complexity\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3040b76",
   "metadata": {},
   "source": [
    "### **6. The Moment of Truth: The Final Evaluation on the Test Set**\n",
    "\n",
    "From the loop and the chart, we can see which `max_leaf_nodes` value gave the best performance on the validation data. Now, and only now, we use the **test set** to get a final, unbiased score for our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38b5470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best number of leaf nodes from our loop of different max_leaf_nodes\n",
    "best_tree_size = # YOUR CODE HERE\n",
    "\n",
    "# Create and train the final model using the best parameter\n",
    "final_model = DecisionTreeClassifier(max_leaf_nodes=best_tree_size, random_state=1)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions on the test set (the data we've never touched before)\n",
    "test_preds = final_model.predict(X_test)\n",
    "\n",
    "# Calculate the final accuracy score\n",
    "final_accuracy = accuracy_score(y_test, test_preds)\n",
    "\n",
    "print(f\"Final Accuracy on the unseen Test Set: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0688b-ec46-4420-913d-e0ab6332231c",
   "metadata": {},
   "source": [
    "## Make it Better!\n",
    "\n",
    "Changing the complexity of the decision tree (`max_leaf_nodes`) is an important way to improve the performance of the model and increase generalizability. However, there are other \"dials\" you can turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13676aea-c426-4416-b40e-c9112b053bda",
   "metadata": {},
   "source": [
    "### Challenge 1: Experiment with Features\n",
    "\n",
    "We chose a specific set of features to train our model, but are they the best ones?\n",
    "\n",
    "Try adding or removing features from your features list.\n",
    "\n",
    "Create a new `X` with your new chosen features and run it through the training and validation process to see if your score improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a51b0b-6b38-4554-8562-231a99af2954",
   "metadata": {},
   "source": [
    "### Challenge 2: Tune Different Hyperparameters\n",
    "\n",
    "We only tuned `max_leaf_nodes`, but Decision Trees have other important settings, called **hyperparameters** you can adjust to control their complexity.\n",
    "\n",
    "`min_samples_leaf`: The minimum number of songs that must be in a final leaf node. A higher number prevents the model from making decisions based on just one or two songs, which helps fight overfitting.\n",
    "\n",
    "`max_depth`: The maximum number of \"if-then\" questions the tree can ask in a row.\n",
    "\n",
    "These hyperparameters can be set with additional arguments after `max_leaf_nodes=` in the `DecisionTreeClassifier` function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
